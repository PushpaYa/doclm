{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a9f8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'menu.price': 0, 'menu.itemsubtotal': 1, 'menu.nm': 2, 'sub_total.service_price': 3, 'total.emoneyprice': 4, 'sub_total.etc': 5, 'total.changeprice': 6, 'total.total_etc': 7, 'menu.discountprice': 8, 'O': 9, 'sub_total.discount_price': 10, 'total.menutype_cnt': 11, 'total.creditcardprice': 12, 'menu.sub_nm': 13, 'menu.sub_cnt': 14, 'menu.num': 15, 'total.cashprice': 16, 'total.total_price': 17, 'total.menuqty_cnt': 18, 'sub_total.subtotal_price': 19, 'menu.cnt': 20, 'menu.unitprice': 21, 'sub_total.tax_price': 22, 'menu.sub_price': 23}\n",
      "/home/pushpa/cord_original/train/image\n",
      "/home/pushpa/cord_original/test/image\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff95ba89b15e44838f19f6f360336c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab295e4c5c44611a543a5549be4a5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import LayoutLMv2Processor\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "from transformers import LayoutLMv2Model\n",
    "from transformers.models.layoutlm import LayoutLMConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torchvision\n",
    "from torchvision.ops import RoIAlign\n",
    "from transformers import LayoutLMv2Tokenizer\n",
    "from torchnlp.nn import Attention\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader,TensorDataset\n",
    "import pprint\n",
    "import numpy\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "dataset_path_train = \"/home/pushpa/cord_original/train/image\"\n",
    "dataset_path_test = \"/home/pushpa/cord_original/test/image\"\n",
    "labels = set()\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os import listdir\n",
    "path = '/home/pushpa/cord_original'\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(path+'/train/changed') if isfile(join(path+'/train/changed', f))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    #print(filename)\n",
    "    annotationpath = path + \"/train/changed/\"+ filename\n",
    "    f = open(annotationpath, encoding=\"utf8\")\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "      # print(item)\n",
    "      labels.add(item['label'])\n",
    "\n",
    "labels = list(labels)\n",
    "\n",
    "idx2label = {v: k for v, k in enumerate(labels)}\n",
    "label2idx = {k: v for v, k in enumerate(labels)}\n",
    "\n",
    "print(label2idx)\n",
    "\n",
    "def normalize_box(box, width, height):\n",
    "     return [\n",
    "         int(100 * (box[0] / width)),\n",
    "         int(100 * (box[1] / height)),\n",
    "         int(100 * (box[2] / width)),\n",
    "         int(100 * (box[3] / height)),\n",
    "     ]\n",
    "# In[2]:\n",
    "# Create dataframe containing image path\n",
    "dirpath = path\n",
    "def create_img_path(dataset_path):\n",
    "    images_train = []\n",
    "    dataset_fldr= dataset_path.split(\"/\")[-2]\n",
    "    for label_folder, _, file_names in os.walk(dataset_path):\n",
    "      \n",
    "      for _, _, image_names in os.walk(label_folder):\n",
    "          print(label_folder)\n",
    "          relative_image_names = []\n",
    "          for image in image_names:\n",
    "            relative_image_names.append( dirpath + \"/\"+dataset_fldr+'/image/'+ image)\n",
    "          images_train.extend(relative_image_names)\n",
    "          #print(relative_image_names)\n",
    "    return images_train\n",
    "\n",
    "image_path = create_img_path(dataset_path_train)\n",
    "traindata_ = pd.DataFrame.from_dict({'image_path': image_path})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_path = create_img_path(dataset_path_test)\n",
    "testdata_ = pd.DataFrame.from_dict({'image_path': image_path})\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "\n",
    "def generate_examples(example):\n",
    "        path = example['image_path']\n",
    "        image = Image.open(path)\n",
    "        w,h = image.size\n",
    "        filename = path.split('/')[-1]\n",
    "        filename = filename.split('.')[0]\n",
    "        datdir=path.split('/')[-3]\n",
    "        #print(datdir)\n",
    "        f = open(dirpath + '/' + datdir + '/changed/'+ filename + '.json')\n",
    "        data = json.load(f)\n",
    "        words = []\n",
    "        boxes = []\n",
    "        neighbors = []\n",
    "        labels = []\n",
    "        #print(data)\n",
    "        for i in data:\n",
    "          words.append(i['text'])\n",
    "          unnormalized_box = i['box']\n",
    "          normalized_box = normalize_box(i['box'],w,h)\n",
    "          #normalized_box = unnormalized_box\n",
    "          # if(i['text'] not in words_dictionary):\n",
    "          #   global id\n",
    "          #   words_dictionary[str(i['text'])] = id\n",
    "          #   box_dictionary[id] = normalized_box\n",
    "          #   id+=1\n",
    "          boxes.append(normalized_box)\n",
    "          neighbors.append(i['neighbors'])\n",
    "          labels.append(label2idx[i['label']])\n",
    "        # normalize the bounding boxes\n",
    "        # add as extra columns \n",
    "        assert len(words) == len(boxes)\n",
    "        example['words'] = words\n",
    "        example['bbox'] = boxes\n",
    "        example['neighbors'] = neighbors\n",
    "        example['labels'] = labels\n",
    "        return example\n",
    "\n",
    "traindata = Dataset.from_pandas(traindata_)\n",
    "train = traindata.map(generate_examples)\n",
    "testdata = Dataset.from_pandas(testdata_)\n",
    "test = testdata.map(generate_examples)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ab7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
    "\n",
    "\n",
    "\n",
    "# we need to define custom features\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'labels': Sequence(feature=Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),     \n",
    "   \n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'neighbours': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  #print(examples['image_path'])\n",
    "  path = examples['image_path']\n",
    "  images = Image.open(path).convert(\"RGB\")\n",
    "  w,h = images.size\n",
    "  images = images.resize((224, 224))\n",
    "  #images = np.asarray(images)  \n",
    "  #images = images[:, :, ::-1] # flip color channels from RGB to BGR\n",
    "  #images = images.transpose(2, 0, 1)\n",
    "  words = examples['words']\n",
    "  #print(examples['bbox'])\n",
    "  bbox = [(normalize_box(x,w,h)) for x in examples['bbox'] ]\n",
    "  #bbox = examples['bbox']\n",
    "  #normalized_box = normalize_box(i['box'],w,h)\n",
    "  word_labels = examples['labels']\n",
    "  \n",
    "  encoded_inputs = processor(images, words, boxes=bbox, word_labels=word_labels,\n",
    "                             padding=\"max_length\", truncation=True)\n",
    "  \n",
    "   \n",
    "\n",
    "  #print( len(encoded_inputs['image']))\n",
    "  del examples['words']\n",
    "  del examples['image_path']\n",
    "  #print(len(encoded_inputs['input_ids']))\n",
    "   # get required padding length\n",
    "  pad_len = 512 - len(examples['neighbors'])\n",
    "  token_neighbors = [[[0]*6]*12]*pad_len\n",
    "  #print(token_neighbors)\n",
    "  examples['neighbors'] = examples['neighbors'] + token_neighbors\n",
    "  \n",
    "  dict1 = {'neighbors':list(examples['neighbors'])}\n",
    "  #dict1 = {'neighbors':examples['neighbors']}\n",
    "  encoded_inputs.update(dict1)\n",
    "  encoded_inputs[\"image\"] = np.array(encoded_inputs[\"image\"])\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  return encoded_inputs\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = []\n",
    "for example in train:\n",
    "    #print(example)\n",
    "    processed_example = preprocess_data(example)\n",
    "    example.update(processed_example)\n",
    "    train_dataset.append(example)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = []\n",
    "for example in test:\n",
    "    #print(example)\n",
    "    processed_example = preprocess_data(example)\n",
    "    example.update(processed_example)\n",
    "    test_dataset.append(example)\n",
    "\n",
    "#encoding['neighbors'][0][0]\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cce8896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/pushpa/cord_original/test/image/receipt_00099.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00067.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00088.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00016.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00019.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00091.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00017.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00015.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00014.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00047.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00095.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00077.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00039.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00073.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00096.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00032.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00030.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00001.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00092.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00070.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00036.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00079.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00063.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00058.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00044.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00089.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00041.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00065.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00042.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00021.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00075.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00031.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00074.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00040.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00078.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00011.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00018.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00050.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00023.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00027.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00098.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00080.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00003.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00060.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00093.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00004.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00090.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00020.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00087.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00068.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00062.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00000.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00071.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00054.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00028.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00097.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00037.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00010.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00085.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00066.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00081.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00035.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00056.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00038.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00072.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00009.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00057.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00059.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00026.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00082.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00002.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00083.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00007.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00013.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00052.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00045.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00043.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00046.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00061.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00029.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00005.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00048.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00051.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00076.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00064.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00055.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00012.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00086.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00033.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00034.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00024.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00053.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00084.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00008.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00006.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00094.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00069.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00022.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00025.png',\n",
       " '/home/pushpa/cord_original/test/image/receipt_00049.png']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train_dataset[0]\n",
    "print(processor.tokenizer.decode(encoding['neighbors'][0][0]))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "   elem = batch[0]\n",
    "   #print(type(elem))\n",
    "   bbox = [item['bbox'] for item in batch]  # just form a list of tensor\n",
    "   neighbours = [item['neighbors'] for item in batch]\n",
    "   labels = [item['labels'] for item in batch]\n",
    "   input_ids = [item['input_ids'] for item in batch]\n",
    "   token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "   attention_mask = [item['attention_mask'] for item in batch]\n",
    "   image =  [item['image'] for item in batch][0]\n",
    "   elem= {'bbox' : torch.tensor(bbox), 'neighbors':torch.tensor(neighbours),'labels':torch.Tensor(labels),'input_ids':torch.Tensor(input_ids),'token_type_ids':torch.Tensor(token_type_ids),'attention_mask':torch.Tensor(attention_mask),'image':torch.Tensor(image)}\n",
    "   #return dict(bbox = bbox, neighbors=neighbors,labels=labels,input_ids=input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask,image=image)\n",
    "   #transposed = zip(*batch)\n",
    "   #return [default_collate(samples) for samples in transposed]\n",
    "   return elem\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0,  collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1,shuffle=True, num_workers=0,  collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5712167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "tokenizer = LayoutLMv2Tokenizer.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
    "class LayoutLMv2ForTokenClassification(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.start_time = time.time()\n",
    "        # LayoutLM base model + token classifier\n",
    "        self.num_labels = len(label2idx)\n",
    "        self.layoutlm = LayoutLMv2Model.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", num_labels=self.num_labels)\n",
    "        \n",
    "        self.mlp = nn.Linear(13*768,12*768)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.attn = Attention(768)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.layoutlm.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(2*768, self.num_labels)\n",
    "\n",
    "    def forward(self,image,input_ids,bbox,attention_mask,token_type_ids,neighbors,inputs_embeds=None,position_ids=None,head_mask=None,\n",
    "        labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.layoutlm.config.use_return_dict\n",
    "        # print('layoutlm Start',time.time()-self.start_time)\n",
    "        # first, forward pass on LayoutLM\n",
    "        #tokenizer = LayoutLMv2Tokenizer.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
    "        outputs = self.layoutlm(\n",
    "            image=image,\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds = inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print('layoutlm End',time.time()-self.start_time)\n",
    "        # print(self.layoutlm.config.hidden_size)\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        sequence_output = outputs[0][:, :seq_length]\n",
    "        dictionary = {}\n",
    "        batchsize = sequence_output.shape[0]\n",
    "        final_neighbors = []\n",
    "        # print('Making Neighbours',time.time()-self.start_time)\n",
    "        for batch in range(batchsize):\n",
    "          for i in range(512):\n",
    "            dictionary[int(input_ids[batch][i])] = sequence_output[batch][i]\n",
    "          l = []\n",
    "          for i in range(512):\n",
    "            # for n in neighbors[batch]:\n",
    "            temp = []\n",
    "            temp = torch.Tensor(temp).cuda()\n",
    "            for j in range(12):\n",
    "              word_tokens = neighbors[batch][i][j]\n",
    "              embed = torch.zeros(768).cuda()\n",
    "              for w in word_tokens:\n",
    "                w = w.item()\n",
    "                if(w in dictionary):\n",
    "                  # print(w)\n",
    "                  embed = torch.add(embed,dictionary[w])\n",
    "                else:\n",
    "                  embed = torch.add(embed,torch.zeros(768).cuda())\n",
    "              temp = torch.cat((temp,embed))\n",
    "              \n",
    "              # print('temp',temp.shape)\n",
    "              # temp = torch.stack(temp)\n",
    "            temp = torch.reshape(temp,(12,768))\n",
    "            l.append(temp)\n",
    "          l = torch.stack(l)\n",
    "          final_neighbors.append(l)\n",
    "        final_neighbors = torch.stack(final_neighbors)\n",
    "        # print('final_neighbors',final_neighbors.shape)\n",
    "        neighbors = final_neighbors.to(device)\n",
    "        # neighbors = final_neighbors\n",
    "        # print('Neighbours Made',time.time()-self.start_time)\n",
    "        # print(neighbors.shape)\n",
    "        # print('Gating and Attention Start',time.time()-self.start_time)\n",
    "        final_output = []\n",
    "        batchsize = sequence_output.shape[0]\n",
    "        for i in range(batchsize): \n",
    "          temp = []\n",
    "          for row,n in zip(sequence_output[i],neighbors[i]):\n",
    "            # print(row.shape,n.shape)\n",
    "            rij = []\n",
    "            rij = torch.Tensor(rij)\n",
    "            rij = rij.to(device)\n",
    "            for z in range(12):\n",
    "              # rij = torch.cat((row,n[z]),0)\n",
    "              rij = torch.cat((rij,torch.cat((row,n[z]),0)),0)\n",
    "            # print(rij.shape)\n",
    "            wr = self.mlp(rij)\n",
    "            # print(wr.shape)\n",
    "            g = self.sigmoid(wr)\n",
    "            # print(g.shape)\n",
    "            g = torch.reshape(g,(12,768))\n",
    "            c_dash = torch.mul(g,n)\n",
    "            # print(c_dash.shape)\n",
    "            c_dash = torch.reshape(c_dash,(12,768))\n",
    "            c_dash = c_dash.unsqueeze(0)\n",
    "            row = row.unsqueeze(0)\n",
    "            row = row.unsqueeze(0)\n",
    "            # print(row.shape)\n",
    "            # print(c_dash.shape)\n",
    "            output,weights = self.attn(row,c_dash)\n",
    "            temp.append(output[0][0])\n",
    "          temp = torch.stack(temp)\n",
    "          # temp = .unsqueeze(0)\n",
    "          # print(final_output.shape)\n",
    "          final_output.append(temp)\n",
    "        final_output = torch.stack(final_output)\n",
    "        final_output = torch.cat((sequence_output,final_output),2)\n",
    "        # print('Gating And Attention End',time.time()-self.start_time)\n",
    "        # print(final_output.shape)\n",
    "        # print('Classification Start',time.time()-self.start_time)\n",
    "        final_output = self.dropout(final_output)\n",
    "        logits = self.classifier(final_output)\n",
    "        # attention_mask = torch.cat((attention_mask,attention_mask),1)\n",
    "        # labels = torch.cat((labels,labels),1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        # print('Classification End',time.time()-self.start_time)\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283112d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17914/4171345256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# kfold = KFold(n_splits=k_folds, shuffle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnum_train_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mt_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_train_epochs\u001b[0m \u001b[0;31m# total number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayoutLMv2ForTokenClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader,TensorDataset\n",
    "import pprint\n",
    "import numpy\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# k_folds = 10\n",
    "results = []\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=False)\n",
    "num_train_epochs = 4\n",
    "t_total = len(train_dataloader) * num_train_epochs # total number of training steps \n",
    "model = LayoutLMv2ForTokenClassification()\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2.373260750776288e-05)\n",
    "# wandb.init(name='First Run', \n",
    "#           project='FinalResultsWithFullDatasetwith45epochs',\n",
    "#           notes='FinalResultsWithFullDataset', \n",
    "#           tags=['FinalResultsWithFullDataset'])\n",
    "global_step = 0\n",
    "# wandb.watch(model)\n",
    "model.train() \n",
    "for epoch in range(num_train_epochs):  \n",
    "  print(\"Epoch:\", epoch)\n",
    "  for batch in tqdm(train_dataloader):\n",
    "        # input_ids = torch.Tensor(list(batch['input_ids'].values)).to(device)\n",
    "        input_ids=batch['input_ids'].long().cpu()\n",
    "        bbox=batch['bbox'].long().cpu()\n",
    "        attention_mask=batch['attention_mask'].long().cpu()\n",
    "        token_type_ids=batch['token_type_ids'].long().cpu()\n",
    "        labels=batch['labels'].long().cpu()\n",
    "        image = batch['image'].long().cpu()\n",
    "        neighbors = batch['neighbors'].long().cpu()\n",
    "        # print(batch)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(labels=labels,\n",
    "                      input_ids= input_ids,\n",
    "                      bbox=bbox, \n",
    "                      attention_mask=attention_mask,\n",
    "                      token_type_ids=token_type_ids,\n",
    "                      image=image,\n",
    "                      neighbors= neighbors)\n",
    "        loss = outputs.loss\n",
    "        # print loss every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "          print(f\"Loss after {global_step} steps: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "\n",
    "save_path = f'/home/pushpa/cord.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "#torch.cuda.empty_cache()\n",
    "model = LayoutLMv2ForTokenClassification()\n",
    "#model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(save_path), strict=False)\n",
    "#device = torch.device('cuda:0')\n",
    "#model = model.to(device)\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "# put model in evaluation mode\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids=batch['input_ids'].long().cpu()\n",
    "        bbox=batch['bbox'].long().cpu()\n",
    "        attention_mask=batch['attention_mask'].long().cpu()\n",
    "        token_type_ids=batch['token_type_ids'].long().cpu()\n",
    "        labels=batch['labels'].long().cpu()\n",
    "        image = batch['image'].long().cpu()\n",
    "        neighbors = batch['neighbors'].long().cpu()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, \n",
    "                        labels=labels,image=image,neighbors=neighbors)\n",
    "        \n",
    "        # predictions\n",
    "        predictions = outputs.logits.argmax(dim=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [idx2label[p.item()] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [idx2label[l.item()] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "final_score = metric.compute()\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds_val = None\n",
    "out_label_ids = None\n",
    "\n",
    "# put model in evaluation mode\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids=batch['input_ids'].long().cpu()\n",
    "        bbox=batch['bbox'].long().cpu()\n",
    "        attention_mask=batch['attention_mask'].long().cpu()\n",
    "        token_type_ids=batch['token_type_ids'].long().cpu()\n",
    "        labels=batch['labels'].long().cpu()\n",
    "        image = batch['image'].long().cpu()\n",
    "        neighbors = batch['neighbors'].long().cpu()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, \n",
    "                        labels=labels,image=image,neighbors=neighbors)\n",
    "        \n",
    "        if preds_val is None:\n",
    "          preds_val = outputs.logits.detach().cpu().numpy()\n",
    "          out_label_ids = batch[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "          preds_val = np.append(preds_val, outputs.logits.detach().cpu().numpy(), axis=0)\n",
    "          out_label_ids = np.append(\n",
    "              out_label_ids, batch[\"labels\"].detach().cpu().numpy(), axis=0\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score)\n",
    "\n",
    "def results_test(preds, out_label_ids, labels):\n",
    "  preds = np.argmax(preds, axis=2)\n",
    "\n",
    "  label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "  out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "  preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "  for i in range(out_label_ids.shape[0]):\n",
    "      for j in range(out_label_ids.shape[1]):\n",
    "          if out_label_ids[i, j] != -100:\n",
    "              out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "              preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "  results = {\n",
    "      \"precision\": precision_score(out_label_list, preds_list),\n",
    "      \"recall\": recall_score(out_label_list, preds_list),\n",
    "      \"f1\": f1_score(out_label_list, preds_list,average='weighted'),\n",
    "  }\n",
    "  return results, classification_report(out_label_list, preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'S-ADDRESS', 'S-TOTAL', 'S-COMPANY', 'S-DATE']\n",
    "val_result, class_report = results_test(preds_val, out_label_ids, labels)\n",
    "print(\"Overall results:\", val_result)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d05359",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d5381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
